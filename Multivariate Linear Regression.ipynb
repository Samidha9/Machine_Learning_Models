{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fad42c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe6ec9",
   "metadata": {},
   "source": [
    "## From Single-Feature to Multi-Feature Linear Regression\n",
    "\n",
    "In single-feature linear regression, each example has one input value and the model is:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = wx + b\n",
    "$$\n",
    "\n",
    "Gradients are computed using simple loops over the training examples.\n",
    "\n",
    "In multi-feature linear regression, each example has multiple input features.  \n",
    "The model becomes:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x^{(i)}) = w \\cdot x^{(i)} + b\n",
    "$$\n",
    "\n",
    "To handle multiple features efficiently, the implementation uses\n",
    "vectorized matrix operations instead of explicit loops.\n",
    "\n",
    "Vectorization makes the code simpler and much faster while producing the same\n",
    "results as the loop-based approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "51564b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    \n",
    "    sigma[sigma == 0] = 1.0\n",
    "    \n",
    "    X_norm = (X - mu) / sigma\n",
    "    return X_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e4612",
   "metadata": {},
   "source": [
    "Normalization helps gradient descent converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4d92a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    y_hat = X @ w + b\n",
    "    cost = (1/(2*m)) * np.sum((y_hat - y)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d2edb1",
   "metadata": {},
   "source": [
    "X @ w is the matrix multiplication of matrix X and vector w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aee3fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    e = (X @ w + b) - y\n",
    "    dj_dw = (1/m) * (X.T @ e)\n",
    "    dj_db = (1/m) * np.sum(e)\n",
    "    return dj_dw, dj_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b44080e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
    "    w = w_in.copy()\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
    "\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1e90195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10) (442,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X_train, y_train = load_diabetes(return_X_y=True)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1b8615d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_final: [ -0.46334361 -11.39253135  24.75884834  15.41678469 -34.90023404\n",
      "  20.47050174   3.56291498   8.06723443  34.69870704   3.22689774]\n",
      "b_final: 152.1334841628958\n"
     ]
    }
   ],
   "source": [
    "m = X_train.shape[0]\n",
    "n = X_train.shape[1]\n",
    "w_init = np.zeros(n)\n",
    "b_init = 0.0\n",
    "alpha = 0.1\n",
    "iters = 3000\n",
    "X_train = zscore_normalize_features(X_train)\n",
    "\n",
    "w_final, b_final = gradient_descent(X_train, y_train, w_init, b_init, alpha, iters)\n",
    "\n",
    "print(\"w_final:\", w_final)\n",
    "print(\"b_final:\", b_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2302f83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE yours: 2859.827649465075\n",
      "MSE sk   : 2859.69634758675\n",
      "Your w, b: [ -0.46334361 -11.39253135  24.75884834  15.41678469 -34.90023404\n",
      "  20.47050174   3.56291498   8.06723443  34.69870704   3.22689774] 152.1334841628958\n",
      "Sk w, b  : [ -0.47612079 -11.40686692  24.72654886  15.42940413 -37.67995261\n",
      "  22.67616277   4.80613814   8.42203936  35.73444577   3.21667372] 152.13348416289594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "sk = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "y_pred_yours = X_train @ w_final + b_final\n",
    "y_pred_sk = sk.predict(X_train)\n",
    "\n",
    "print(\"MSE yours:\", np.mean((y_pred_yours - y_train)**2))\n",
    "print(\"MSE sk   :\", np.mean((y_pred_sk - y_train)**2))\n",
    "\n",
    "print(\"Your w, b:\", w_final, b_final)\n",
    "print(\"Sk w, b  :\", sk.coef_, sk.intercept_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
