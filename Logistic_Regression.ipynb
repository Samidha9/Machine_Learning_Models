{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51a95c4",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "Logistic regression keeps the **linear score**:\n",
    "\n",
    "$$\n",
    "z = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "but applies the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "The final model is:\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = \\sigma(w \\cdot x + b)\n",
    "$$\n",
    "\n",
    "This maps any real-valued input into the interval \\((0,1)\\),\n",
    "allowing the output to be interpreted as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6bb6e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354c903",
   "metadata": {},
   "source": [
    "Why the Sigmoid Function?\n",
    "\n",
    "It maps any real-valued input to the interval $ (0, 1) $\n",
    "\n",
    "This allows the model output to be interpreted as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5c13e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cda89806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    \n",
    "    sigma[sigma == 0] = 1.0\n",
    "    \n",
    "    X_norm = (X - mu) / sigma\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d050b",
   "metadata": {},
   "source": [
    "#### Decision Boundary\n",
    "\n",
    "Predictions are made using the rule:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } f_{w,b}(x) \\ge 0.5 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since:\n",
    "\n",
    "$$\n",
    "\\sigma(z) \\ge 0.5 \\iff z \\ge 0\n",
    "$$\n",
    "\n",
    "the decision boundary is defined by:\n",
    "\n",
    "$$\n",
    "w \\cdot x + b = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e5d42",
   "metadata": {},
   "source": [
    "#### Cross-Entropy Loss\n",
    "\n",
    "Logistic regression models a Bernoulli distribution for the target variable.\n",
    "\n",
    "The cost function is:\n",
    "\n",
    "$$\n",
    "J(w,b) =\n",
    "-\\frac{1}{m}\n",
    "\\sum_{i=1}^{m}\n",
    "\\left[\n",
    "y^{(i)} \\log(\\hat{y}^{(i)}) +\n",
    "(1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This function measures how badly the modelâ€™s predicted probabilities disagree with the actual outcomes, heavily penalizing confident wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "93541242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    y_pred = np.zeros(m)\n",
    "\n",
    "    for i in range(m):\n",
    "        y_pred[i] = sigmoid(np.dot(X[i], w) + b)\n",
    "\n",
    "    # avoid log(0)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    total_cost = np.sum(-y*np.log(y_pred) - (1-y)*np.log(1-y_pred)) / m\n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2241770",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "The gradients of the loss function are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w}\n",
    "=\n",
    "\\frac{1}{m} X^T (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b}\n",
    "=\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "These expressions closely resemble linear regression, with a different\n",
    "interpretation of the error term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e0de9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    m = X.shape[0]\n",
    "    y_hat = sigmoid(X @ w + b)\n",
    "    error = y_hat - y\n",
    "    dj_dw = (1/m) * (X.T @ error)\n",
    "    dj_db = (1/m) * np.sum(error)\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4e411777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, compute_gradient, alpha, num_iters): \n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        dj_db, dj_dw = compute_gradient(X, y, w_in, b_in)   \n",
    "\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "\n",
    "        \n",
    "    return w_in, b_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dc8bd742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    for i in range(m):   \n",
    "        z_wb = 0\n",
    "        for j in range(n): \n",
    "            z_wb += X[i, j] * w[j]\n",
    "    \n",
    "        z_wb += b\n",
    "        \n",
    "        f_wb = sigmoid(z_wb)\n",
    "\n",
    "        p[i] = 0 if f_wb < 0.5 else 1\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d163e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (569, 30) y shape: (569,) positive rate: 0.6274165202108963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target.astype(float)  \n",
    "\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape, \"positive rate:\", y.mean())\n",
    "\n",
    "X_train = zscore_normalize_features(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "eff2a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train.shape[1]\n",
    "w0 = np.zeros(n, dtype=float)\n",
    "b0 = 0.0\n",
    "\n",
    "w_fit, b_fit = gradient_descent(\n",
    "    X_train, y, w0, b0, compute_gradient, alpha=0.1, num_iters=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a802d668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9876977152899824\n"
     ]
    }
   ],
   "source": [
    "yhat_train = predict(X_train, w_fit, b_fit)\n",
    "\n",
    "print(\"Train accuracy:\", (yhat_train == y).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d16a5907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[207   5]\n",
      " [  2 355]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9904    0.9764    0.9834       212\n",
      "         1.0     0.9861    0.9944    0.9902       357\n",
      "\n",
      "    accuracy                         0.9877       569\n",
      "   macro avg     0.9883    0.9854    0.9868       569\n",
      "weighted avg     0.9877    0.9877    0.9877       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y, yhat_train)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y, yhat_train, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888fdf9",
   "metadata": {},
   "source": [
    "Precision: correctness of positive predictions.  \n",
    "Recall: coverage of actual positive cases.  \n",
    "F1-score: balance between precision and recall.  \n",
    "Support: number of true samples per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aa4459ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn Train accuracy: 0.9876977152899824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sk = LogisticRegression(max_iter=2000)\n",
    "sk.fit(X_train, y)\n",
    "\n",
    "print(\"sklearn Train accuracy:\", sk.score(X_train, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
